{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d88639c1aadf1067",
   "metadata": {},
   "source": [
    "## 数据预处理\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "7a6af711653f5ac0",
   "metadata": {},
   "source": [
    "import os.path\n",
    "\n",
    "train_data = open('../data/raw/sogou_rematch/user_tag_query.10W.TRAIN', 'r', encoding='gb18030')\n",
    "OUTPUT_DATA_PATH = '../data/processed/query_words.train'\n",
    "\n",
    "try:\n",
    "    if os.path.exists(OUTPUT_DATA_PATH):\n",
    "        os.remove(OUTPUT_DATA_PATH)\n",
    "\n",
    "    output_data = open(OUTPUT_DATA_PATH, 'w', encoding='utf-8')\n",
    "\n",
    "    for line in train_data:\n",
    "        line_list = line.split('\\t') # 以制表符分割\n",
    "        line_list = line_list[4:] # 只保留查询词\n",
    "        output_line = '\\n'.join(line_list) # 以换行符连接\n",
    "        output_data.write(output_line) # 写入文件\n",
    "        \n",
    "finally:\n",
    "    train_data.close()\n",
    "    output_data.close()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "fc9750c11cfd34b",
   "metadata": {},
   "source": [
    "import re\n",
    "import os.path\n",
    "\n",
    "OUTPUT_DATA_PATH = '../data/processed/cleaned.train'\n",
    "\n",
    "try:\n",
    "    train_data = open('../data/processed/query_words.train', 'r', encoding='utf-8')\n",
    "    if os.path.exists(OUTPUT_DATA_PATH):\n",
    "        os.remove(OUTPUT_DATA_PATH)\n",
    "\n",
    "    output_data = open(OUTPUT_DATA_PATH, 'w', encoding='utf-8')\n",
    "\n",
    "    for line in train_data:\n",
    "        word_list = line.split('\\t')\n",
    "        pattern = re.compile('(https?|ftp|file)://[-A-Za-z0-9+&@#/%?=~_|!:,.;]+[-A-Za-z0-9+&@#/%=~_|]')\n",
    "        # 如果字符串包含网址，则跳过\n",
    "        if pattern.search(word_list[0]):\n",
    "            continue\n",
    "        line_string = '\\n'.join(word_list)\n",
    "        output_data.write(line_string)\n",
    "finally:\n",
    "    train_data.close()\n",
    "    output_data.close()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e5773289bf4f5f23",
   "metadata": {},
   "source": [
    "## 分词"
   ]
  },
  {
   "cell_type": "code",
   "id": "fa34e3609b4450a5",
   "metadata": {},
   "source": [
    "import jieba\n",
    "# 读取cleaned.train文件\n",
    "train_data = open('../data/processed/cleaned.train', 'r', encoding='utf-8')\n",
    "output_data = open('../data/processed/seg_list.train', 'w', encoding='utf-8')\n",
    "# 逐行处理\n",
    "for line in train_data:\n",
    "    line = line.strip()\n",
    "    temp_seg_list = list(jieba.cut(line))\n",
    "    # 去除空格\n",
    "    temp_seg_list = [word for word in temp_seg_list if word != ' ']\n",
    "    output_data.write('\\n'.join(temp_seg_list))\n",
    "    # 换行\n",
    "    output_data.write('\\n')\n",
    "# 关闭文件\n",
    "print('分词处理完成！')\n",
    "train_data.close()\n",
    "output_data.close()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 合并停用词文件",
   "id": "c6ab45806ae99c03"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 停用词文件路径\n",
    "stopwords_file_1 = '../data/stop_words/baidu_stopwords.txt'  # 第一个停用词文件\n",
    "stopwords_file_2 = '../data/stop_words/cn_stopwords.txt'  # 第二个停用词文件\n",
    "output_file = '../data/stop_words/merge_stopwords.txt'  # 输出合并后的停用词文件\n",
    "\n",
    "# 1. 读取第一个停用词文件\n",
    "stopwords = set()  # 使用集合以避免重复\n",
    "\n",
    "with open(stopwords_file_1, 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        stopwords.add(line.strip())  # 去除行首尾空白并添加到集合中\n",
    "\n",
    "# 2. 读取第二个停用词文件\n",
    "with open(stopwords_file_2, 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        stopwords.add(line.strip())  # 去除行首尾空白并添加到集合中\n",
    "\n",
    "# 3. 将合并后的停用词写入输出文件\n",
    "with open(output_file, 'w', encoding='utf-8') as file:\n",
    "    for word in sorted(stopwords):  # 可选：按字母顺序排序\n",
    "        file.write(word + '\\n')  # 每个停用词写入一行"
   ],
   "id": "39e2c3cc75837deb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 过滤词",
   "id": "dd6e38acd85cdd91"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 1. 加载停用词\n",
    "stopwords_file = '../data/stop_words/merge_stopwords.txt'  # 停用词文件路径\n",
    "stopwords = set()\n",
    "\n",
    "with open(stopwords_file, 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        stopwords.add(line.strip())\n",
    "\n",
    "# 2. 读取已分词的训练数据并过滤停用词\n",
    "train_file = '../data/processed/seg_list.train'  # 已分词的训练数据路径\n",
    "output_file = '../data/processed/filter_list.train'  # 过滤后的训练数据路径\n",
    "\n",
    "with open(train_file, 'r', encoding='utf-8') as train_data, \\\n",
    "     open(output_file, 'w', encoding='utf-8') as output_data:\n",
    "\n",
    "    for line in train_data:\n",
    "        line = line.strip()  # 去除行首尾空白\n",
    "        words = line.split()  # 将分词结果按空格拆分\n",
    "        # 过滤停用词\n",
    "        filtered_words = [word for word in words if word not in stopwords]\n",
    "        if filtered_words:  # 确保不写入空行\n",
    "            output_data.write(' '.join(filtered_words) + '\\n')  # 以空格连接过滤后的词\n"
   ],
   "id": "b836a95bbcd1e7b9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 选取关键词",
   "id": "5c4a53c64427f7aa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import time\n",
    "from collections import Counter\n",
    "\n",
    "def read_and_count_words(filename, exclude_single=True, exclude_specific=None):\n",
    "    # 记录开始时间\n",
    "    start_time = time.time()\n",
    "\n",
    "    # 读取文件内容并将每一行作为一个单词\n",
    "    with open(filename, 'r', encoding='utf-8') as file:\n",
    "        words = [line.strip() for line in file]\n",
    "    \n",
    "    # 去除单个字的词语\n",
    "    if exclude_single:\n",
    "        words = [word for word in words if len(word) > 1]\n",
    "    \n",
    "    # 去除特定的词语\n",
    "    if exclude_specific:\n",
    "        words = [word for word in words if word not in exclude_specific]\n",
    "    \n",
    "    # 使用Counter来统计每个词的出现频率\n",
    "    word_counts = Counter(words)\n",
    "    \n",
    "    # 获取出现频率最高的前20个词语\n",
    "    most_common_20 = word_counts.most_common(20)\n",
    "    \n",
    "    # 记录结束时间\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    \n",
    "    print(f\"运行时间: {elapsed_time:.4f}秒\")\n",
    "    \n",
    "    return most_common_20\n",
    "\n",
    "# 调用函数，并打印结果\n",
    "filename = '../data/processed/filter_list.train'  \n",
    "exclude_specific = ['2016']\n",
    "most_common_20 = read_and_count_words(filename, exclude_single=True, exclude_specific=exclude_specific)\n",
    "\n",
    "# 打印出前20个最常见的词语及其频率\n",
    "for word, freq in most_common_20:\n",
    "    print(f'{word}: {freq}')"
   ],
   "id": "cc44368ea2891478",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 哈希+最小堆",
   "id": "c8cd6d12ecddbeef"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import time\n",
    "import heapq\n",
    "\n",
    "def read_and_count_words(filename, exclude_single=True, exclude_specific=None):\n",
    "    # 初始化一个空的计数字典\n",
    "    word_counts = {}\n",
    "    \n",
    "    # 记录开始时间\n",
    "    start_time = time.time()\n",
    "\n",
    "    # 读取文件内容并将每一行作为一个单词\n",
    "    with open(filename, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            word = line.strip()\n",
    "            # 检查是否需要排除单个字符的单词或特定单词\n",
    "            if (not exclude_single or len(word) > 1) and (exclude_specific is None or word not in exclude_specific):\n",
    "                # 更新计数字典\n",
    "                word_counts[word] = word_counts.get(word, 0) + 1\n",
    "    \n",
    "    # 创建一个大小为20的最小堆\n",
    "    heap = []\n",
    "    for word, count in word_counts.items():\n",
    "        if len(heap) < 20:\n",
    "            heapq.heappush(heap, (count, word))\n",
    "        else:\n",
    "            # 如果当前单词的计数大于堆顶元素，则替换它\n",
    "            if count > heap[0][0]:\n",
    "                heapq.heapreplace(heap, (count, word))\n",
    "    \n",
    "    # 将堆转换成有序列表\n",
    "    most_common_20 = [(word, count) for count, word in sorted(heap, reverse=True)]\n",
    "    \n",
    "    # 记录结束时间\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    \n",
    "    print(f\"运行时间: {elapsed_time:.4f}秒\")\n",
    "    \n",
    "    return most_common_20\n",
    "\n",
    "# 调用函数，并打印结果\n",
    "filename = '../data/processed/filter_list.train'  \n",
    "exclude_specific = ['2016']\n",
    "most_common_20 = read_and_count_words(filename, exclude_single=True, exclude_specific=exclude_specific)\n",
    "\n",
    "# 打印出前20个最常见的词语及其频率\n",
    "for word, freq in most_common_20:\n",
    "    print(f'{word}: {freq}')"
   ],
   "id": "7e8e85eb5755cdba",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 筛选含有种子关键词的搜索条目——暴力匹配",
   "id": "a3668e07296e4afb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import time\n",
    "\n",
    "def read_seed_words(filename):\n",
    "    # 从指定文件中读取种子词\n",
    "    with open(filename, 'r', encoding='utf-8') as file:\n",
    "        return [line.strip() for line in file]\n",
    "\n",
    "def filter_items_with_seed_words(input_filename, output_filename, seed_words):\n",
    "    # 创建一个字典来存储每个种子词对应的数据项\n",
    "    categorized_items = {seed_word: [] for seed_word in seed_words}\n",
    "    \n",
    "    # 读取输入文件并分类数据项\n",
    "    with open(input_filename, 'r', encoding='gbk', errors='ignore') as infile:\n",
    "        for line in infile:\n",
    "            items = line.strip().split()  # 使用空格分割每一行的数据项\n",
    "            \n",
    "            # 检查每一个数据项\n",
    "            for item in items:\n",
    "                for seed_word in seed_words:\n",
    "                    if seed_word in item:\n",
    "                        # 添加到对应的种子词列表中\n",
    "                        categorized_items[seed_word].append(item)\n",
    "    \n",
    "    # 写入输出文件\n",
    "    with open(output_filename, 'w', encoding='utf-8') as outfile:\n",
    "        # 遍历每个种子词及其对应的数据项\n",
    "        for seed_word, items in categorized_items.items():\n",
    "            if items:\n",
    "                outfile.write(f\"{seed_word}::\\n\")\n",
    "                for item in items:\n",
    "                    outfile.write(f\"  {item}\\n\")\n",
    "\n",
    "# 主函数\n",
    "if __name__ == \"__main__\":\n",
    "    # 记录开始时间\n",
    "    start_time = time.time()\n",
    "\n",
    "    # 指定文件路径\n",
    "    seed_words_file = '../data/seed_words'\n",
    "    user_tag_query_file = '../data/user_tag_query.10W.TRAIN'\n",
    "    output_file = '../data/seed_words_query.train'\n",
    "\n",
    "    # 读取种子词\n",
    "    seed_words = read_seed_words(seed_words_file)\n",
    "    # print(\"读取的种子词:\", seed_words)\n",
    "\n",
    "    # 过滤并分类保存结果\n",
    "    filter_items_with_seed_words(user_tag_query_file, output_file, seed_words)\n",
    "    \n",
    "    # 记录结束时间\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"程序运行时间: {elapsed_time:.4f}秒\")\n",
    "    \n",
    "    # print(f\"筛选后的数据已保存至 {output_file}\")"
   ],
   "id": "8d1c72c7c275cfbd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 筛选含有种子关键词的搜索条目——ac自动机",
   "id": "c29805205761b577"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import time\n",
    "from ahocorasick import Automaton\n",
    "\n",
    "def read_seed_words(filename):\n",
    "    # 从指定文件中读取种子词\n",
    "    with open(filename, 'r', encoding='utf-8') as file:\n",
    "        return [line.strip() for line in file]\n",
    "\n",
    "def match_patterns_in_file(input_filename, output_filename, automaton):\n",
    "    # 使用自动机匹配文本中的模式串，并将结果写入到新文件中\n",
    "    with open(input_filename, 'r', encoding='gbk', errors='ignore') as input_file, \\\n",
    "         open(output_filename, 'w', encoding='utf-8') as output_file:\n",
    "        for line in input_file:\n",
    "            words = line.split()  # 分割每行的单词\n",
    "            for word in words:\n",
    "                matches = list(automaton.iter(word))\n",
    "                for end_index, (insert_order, original_value) in matches:\n",
    "                    start_index = end_index - len(original_value) + 1\n",
    "                    output_file.write(f\"{word}\\n\")\n",
    "\n",
    "# 文件路径\n",
    "seed_words_file = '../data/seed_words'\n",
    "user_tag_query_file = '../data/user_tag_query.10W.TRAIN'\n",
    "output_file = '../data/seed_words_query_ac.train'\n",
    "\n",
    "# 开始计时\n",
    "start_time = time.time()\n",
    "\n",
    "# 读取种子词\n",
    "seed_words = read_seed_words(seed_words_file)\n",
    "\n",
    "# 创建AC自动机实例\n",
    "automaton = Automaton()\n",
    "\n",
    "# 添加模式串到自动机中\n",
    "for idx, keyword in enumerate(seed_words):\n",
    "    automaton.add_word(keyword, (idx, keyword))  # 可以附加任何数据到关键词上\n",
    "\n",
    "# 构建自动机\n",
    "automaton.make_automaton()\n",
    "\n",
    "# 匹配文件中的所有字符串并将结果输出到新文件中\n",
    "match_patterns_in_file(user_tag_query_file, output_file, automaton)\n",
    "\n",
    "# 结束计时并打印执行时间\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "print(f\"程序执行时间: {execution_time:.2f} 秒\")"
   ],
   "id": "d9ded5955aa328f3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 筛选含种子关键词的搜索条目——WM算法（不适用",
   "id": "7388d7ad8ff9ac25"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import time\n",
    "\n",
    "def create_bad_character_table(pattern):\n",
    "    # 创建“坏字符”位移表\n",
    "    bc = {}\n",
    "    m = len(pattern)\n",
    "    for i in range(m - 1):\n",
    "        # 如果字符不在表中，则设置位移量为模式的长度减去字符的位置减一\n",
    "        bc[pattern[i]] = m - i - 1\n",
    "    return bc\n",
    "\n",
    "def search_patterns_in_file(input_filename, output_filename, patterns):\n",
    "    # 创建每个模式的“坏字符”位移表\n",
    "    bad_char_shift = {pattern: create_bad_character_table(pattern) for pattern in patterns}\n",
    "    \n",
    "    with open(input_filename, 'r', encoding='gbk', errors='ignore') as input_file, \\\n",
    "         open(output_filename, 'w', encoding='utf-8') as output_file:\n",
    "        for line in input_file:\n",
    "            text = line.strip()\n",
    "            words = text.split()  # 按空格分割每行的单词\n",
    "            \n",
    "            for word in words:\n",
    "                for pattern in patterns:\n",
    "                    m = len(pattern)\n",
    "                    n = len(word)\n",
    "                    i = m - 1\n",
    "                    while i < n:\n",
    "                        j = m - 1\n",
    "                        while j >= 0 and word[i] == pattern[j]:\n",
    "                            if j == 0:\n",
    "                                output_file.write(f\"{word}\\n\")\n",
    "                                break\n",
    "                            i -= 1\n",
    "                            j -= 1\n",
    "                        if j > 0:  # 没有完全匹配\n",
    "                            shift = bad_char_shift[pattern].get(word[i], m)\n",
    "                            i += shift\n",
    "                        else:\n",
    "                            i += 1\n",
    "\n",
    "# 文件路径\n",
    "seed_words_file = '../data/seed_words'\n",
    "user_tag_query_file = '../data/user_tag_query.10W.TRAIN'\n",
    "output_file = '../data/seed_words_query_wu_manber.train'\n",
    "\n",
    "# 开始计时\n",
    "start_time = time.time()\n",
    "\n",
    "# 读取种子词\n",
    "with open(seed_words_file, 'r', encoding='utf-8') as file:\n",
    "    seed_words = [line.strip() for line in file]\n",
    "\n",
    "# 在文件中搜索模式并将结果输出到新文件中\n",
    "search_patterns_in_file(user_tag_query_file, output_file, seed_words)\n",
    "\n",
    "# 结束计时并打印执行时间\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "print(f\"程序执行时间: {execution_time:.2f} 秒\")"
   ],
   "id": "82824ecdd353c41a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 画图",
   "id": "7a4710921aed09bd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 更新时间消耗数据\n",
    "methods = ['most_common', 'min_heap', 'tim_sort']\n",
    "time_costs = [18.643, 16.864, 17.737]  # 单位秒\n",
    "space_complexities = ['space:O(n)', 'space:O(K)', 'space:O(n)']  # 复杂度\n",
    "\n",
    "# 创建柱状图\n",
    "plt.figure(figsize=(5, 3))  # 增加宽度以容纳第三个条目\n",
    "bar_plot = plt.bar(methods, time_costs, color=['skyblue', 'salmon', 'lightgreen'])\n",
    "\n",
    "# 添加数值标签和空间复杂度标签\n",
    "for index, value in enumerate(time_costs):\n",
    "    plt.text(index, value, f'{value}s\\n{space_complexities[index]}',\n",
    "             ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "# 设置标题和坐标轴标签\n",
    "plt.title('Time Consumption Comparison Between Methods')\n",
    "plt.xlabel('Methods')\n",
    "plt.ylabel('Time Consumption (seconds)')\n",
    "\n",
    "# 调整y轴的范围以确保文本标签不会被截断\n",
    "plt.ylim(0, max(time_costs) * 1.2)\n",
    "\n",
    "# 显示图表\n",
    "plt.show()"
   ],
   "id": "b5ff82e07f327ca",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 画图",
   "id": "a1d9bb9da061e061"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "\n",
    "# 注册字体\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']  # 使用黑体\n",
    "plt.rcParams['axes.unicode_minus'] = False  # 正常显示负号\n",
    "\n",
    "# 时间消耗数据\n",
    "algorithms = ['暴力匹配算法', 'AC自动机算法']\n",
    "time_costs = [14.9673, 11.40]  # 单位秒\n",
    "\n",
    "# 创建柱状图\n",
    "plt.figure(figsize=(4, 3))\n",
    "bar_plot = plt.bar(algorithms, time_costs, color=['skyblue', 'salmon'])\n",
    "\n",
    "# 添加数值标签\n",
    "for index, value in enumerate(time_costs):\n",
    "    plt.text(index, value, f'{value}s',\n",
    "             ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "# 设置标题和坐标轴标签\n",
    "plt.title('算法时间消耗对比')\n",
    "plt.xlabel('算法')\n",
    "plt.ylabel('时间消耗 (秒)')\n",
    "\n",
    "# 调整y轴的范围以确保文本标签不会被截断\n",
    "plt.ylim(0, max(time_costs) * 1.2)\n",
    "\n",
    "# 显示图表\n",
    "plt.show()"
   ],
   "id": "1612bc7e2336aa92",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 搜索条目分词",
   "id": "6c052576528a1387"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import jieba\n",
    "\n",
    "# 关键词列表\n",
    "keywords = [\n",
    "    \"图片::\", \"手机::\", \"小说::\", \"视频::\", \"下载::\", \"大全::\", \"qq::\", \"电影::\", \"中国::\", \"世界::\",\n",
    "    \"重生::\", \"百度::\", \"官网::\", \"txt::\", \"英语::\", \"电视剧::\", \"游戏::\", \"查询::\", \"做法::\", \"倾城::\"\n",
    "]\n",
    "\n",
    "# 去掉冒号后的关键词列表\n",
    "keywords_clean = [keyword[:-2] for keyword in keywords]\n",
    "\n",
    "# 定义一个字典来存储每个词及其对应的分词结果\n",
    "word_dict = {keyword: [] for keyword in keywords_clean}\n",
    "\n",
    "# 当前处理的关键词\n",
    "current_keyword = None\n",
    "\n",
    "# 输入文件路径\n",
    "input_file_path = '../data/processed/seed_words_query.train'\n",
    "# 输出文件路径\n",
    "output_file_path = '../data/processed/seg_mid.train'\n",
    "\n",
    "# 读取输入文件\n",
    "with open(input_file_path, 'r', encoding='utf-8') as train_data:\n",
    "    # 逐行处理\n",
    "    for line in train_data:\n",
    "        line = line.strip()\n",
    "        \n",
    "        # 判断是否为新关键词行，关键词为包含了::\n",
    "        if any(keyword == line for keyword in keywords):\n",
    "            current_keyword = line[:-2]  # 去掉冒号\n",
    "        else:\n",
    "            # 进行分词\n",
    "            seg_list = [word for word in jieba.cut(line) if word != '']\n",
    "            \n",
    "            # 去掉与当前关键词相同的部分\n",
    "            filtered_segs = [seg for seg in seg_list if seg != current_keyword]\n",
    "            if current_keyword and filtered_segs:\n",
    "                word_dict[current_keyword].extend(filtered_segs)\n",
    "\n",
    "# 打开输出文件\n",
    "with open(output_file_path, 'w', encoding='utf-8') as output_data:\n",
    "    # 按词打印分词结果\n",
    "    for keyword, segs in word_dict.items():\n",
    "        output_data.write(f\"{keyword}:\\n\")\n",
    "        for seg in segs:\n",
    "            output_data.write(f\" {seg}\\n\")\n",
    "            \n",
    "print('分词处理完成！')"
   ],
   "id": "7304154ec626edd6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 过滤停用词（中介）",
   "id": "1ecc7c8204db1606"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 1. 加载停用词\n",
    "stopwords_file = '../data/stop_words/merge_stopwords.txt'  # 停用词文件路径\n",
    "stopwords = set()\n",
    "\n",
    "with open(stopwords_file, 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        stopwords.add(line.strip())\n",
    "\n",
    "# 2. 读取已分词的训练数据并过滤停用词\n",
    "train_file = '../data/processed/seg_mid.train'  # 已分词的训练数据路径\n",
    "output_file = '../data/processed/filter_list_mid.train'  # 过滤后的训练数据路径\n",
    "\n",
    "with open(train_file, 'r', encoding='utf-8') as train_data, \\\n",
    "     open(output_file, 'w', encoding='utf-8') as output_data:\n",
    "\n",
    "    for line in train_data:\n",
    "        line = line.strip()  # 去除行首尾空白\n",
    "        words = line.split()  # 将分词结果按空格拆分\n",
    "        # 过滤停用词\n",
    "        filtered_words = [word for word in words if word not in stopwords]\n",
    "        if filtered_words:  # 确保不写入空行\n",
    "            output_data.write(' '.join(filtered_words) + '\\n')  # 以空格连接过滤后的词"
   ],
   "id": "f6db9d1364ca0e9b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 选取中介关键词",
   "id": "ec90c5eb2b1af6e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from collections import Counter\n",
    "keywords = [\n",
    "    \"图片:\", \"手机:\", \"小说:\", \"视频:\", \"下载:\", \"大全:\", \"QQ:\", \"电影:\", \"中国:\", \"世界:\", \n",
    "    \"重生:\", \"百度:\", \"官网:\", \"txt:\", \"英语:\", \"电视剧:\", \"游戏:\", \"查询:\", \"做法:\", \"倾城:\"\n",
    "]\n",
    "# 输入文件路径\n",
    "input_file_path = '../data/processed/filter_list_mid.train'\n",
    "# 输出文件路径\n",
    "output_file_path = '../data/processed/seed_mid.train'\n",
    "current_keyword = None\n",
    "words=[]\n",
    "with open(output_file_path, 'w', encoding='utf-8') as output_data:\n",
    "    pass  # 使用 'w' 模式打开文件并立即关闭，达到清空文件的目的\n",
    "\n",
    "with open(input_file_path, 'r', encoding='utf-8') as train_data:\n",
    "    # 逐行处理\n",
    "    for line in train_data:\n",
    "        line = line.strip()\n",
    "        # 判断是否为新关键词行\n",
    "        if any(keyword == line for keyword in keywords):\n",
    "            if current_keyword is None:\n",
    "                current_keyword = line\n",
    "            else:\n",
    "                word_counts = Counter(words)\n",
    "                most_common_20 = word_counts.most_common(20)\n",
    "                with open(output_file_path, 'a', encoding='utf-8') as output_data:\n",
    "                    output_data.write(current_keyword+ '\\n')\n",
    "                    for word,freq in most_common_20:\n",
    "                        output_data.write(f\" {word} {freq}\\n\")\n",
    "                words.clear()\n",
    "                print(\"clear\"+current_keyword)\n",
    "                current_keyword = line\n",
    "                print(\"new\"+current_keyword)\n",
    "        else:\n",
    "            words.append(line)\n",
    " \n",
    "print(words)           \n",
    "with open(output_file_path, 'a', encoding='utf-8') as output_data:\n",
    "    output_data.write(current_keyword + '\\n')\n",
    "    word_counts = Counter(words)\n",
    "    most_common_20 = word_counts.most_common(20)\n",
    "    for word,freq in most_common_20:\n",
    "        output_data.write(f\" {word} {freq}\\n\")\n",
    "            "
   ],
   "id": "acb3d96266036992",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 统计包含种子关键词和中介关键词的查询搜索量",
   "id": "cdfbb9bb801e9519"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 关键词列表\n",
    "# 关键词列表\n",
    "keywords = [\n",
    "    \"图片::\", \"手机::\", \"小说::\", \"视频::\", \"下载::\", \"大全::\", \"qq::\", \"电影::\", \"中国::\", \"世界::\",\n",
    "    \"重生::\", \"百度::\", \"官网::\", \"txt::\", \"英语::\", \"电视剧::\", \"游戏::\", \"查询::\", \"做法::\", \"倾城::\"\n",
    "]\n",
    "# 去掉了两个冒号的关键词列表\n",
    "keywords_clean = [keyword[:-2] for keyword in keywords]\n",
    "# 去掉了一个冒号的关键词列表\n",
    "seed_words = [keyword[:-1] for keyword in keywords]\n",
    "\n",
    "# 统计包含种子关键词的查询搜索量（不一定包含了种子关键词）\n",
    "# 读取搜索记录文件\n",
    "input_file_path = '../data/processed/seed_words_query.train'\n",
    "# 创建一个字典，用于存储包含种子关键词的搜索记录数\n",
    "seed_dist = {keyword:0 for keyword in keywords_clean}\n",
    "# 读取seed_words_query文件，逐行处理\n",
    "with open(input_file_path, 'r', encoding='utf-8') as input_data:\n",
    "    current_seed_keyword = None\n",
    "    for line in input_data:\n",
    "        line = line.strip()\n",
    "        # 读取到新的关键词\n",
    "        if any(keyword == line for keyword in keywords):\n",
    "            # 重新赋值\n",
    "            current_seed_keyword = line[:-2]\n",
    "        else:\n",
    "            # 统计包含种子关键词的搜索条目的数量\n",
    "            seed_dist[current_seed_keyword] += 1\n",
    "print(seed_dist)\n",
    "\n",
    "# 统计包含了种子关键词和中介关键词的查询搜索量\n",
    "seed_mid_file_path = '../data/processed/seed_mid.train'\n",
    "mid_dist = {keyword:{} for keyword in keywords_clean}\n",
    "with open(seed_mid_file_path, 'r', encoding='utf-8') as seed_mid_data:\n",
    "    current_seed_keyword = None\n",
    "    for line in seed_mid_data:\n",
    "        line = line.strip()\n",
    "        if any(sed_word == line for sed_word in seed_words):\n",
    "            current_seed_keyword = line[:-1]\n",
    "        else:\n",
    "           # 按空格将行分开，第一个元素是中介关键词，第二个元素是词频\n",
    "            split_line = line.split()\n",
    "            mid_keyword = split_line[0]\n",
    "            freq = int(split_line[1])\n",
    "            mid_dist[current_seed_keyword][mid_keyword] = {'freq': freq}\n",
    "print(mid_dist)"
   ],
   "id": "2d4c9eb4aea9c0f0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 确定中介关键词的权重",
   "id": "58c8e3ba96fe19e6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 定义导出路径\n",
    "output_file_path = '../data/processed/seed_mid.train'\n",
    "# 计算权重，同时将结果写入到文件中去\n",
    "with open(output_file_path, 'w', encoding='utf-8') as output_data:\n",
    "    for seed_word, mid_word in mid_dist.items():\n",
    "        output_data.write(seed_word +\":\"+ '\\n')\n",
    "        for mid_keyword, freq in mid_word.items():\n",
    "            mid_word[mid_keyword]['weight'] = freq['freq'] / seed_dist[seed_word]\n",
    "            output_data.write(' ' + mid_keyword + ' ' + str(freq['freq']) + ' '+ str(mid_word[mid_keyword]['weight']) +'\\n')"
   ],
   "id": "715df2cf025c6293",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "5e725457b9783ef2",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
