{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## NLPIR",
   "id": "519440f74cd6922c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pynlpir\n",
    "\n",
    "# 打开分词器\n",
    "pynlpir.open()\n",
    "\n",
    "\n",
    "# 读取文件内容\n",
    "def read_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        return f.read().strip()\n",
    "\n",
    "\n",
    "# 分词函数\n",
    "def segment_text(text):\n",
    "    segments = pynlpir.segment(text, pos_tagging=False)\n",
    "    words = []\n",
    "\n",
    "    for segment in segments:\n",
    "        if isinstance(segment, tuple):\n",
    "            word, _ = segment  # 解包元组，只提取词\n",
    "            words.append(word)\n",
    "        elif isinstance(segment, str):\n",
    "            words.append(segment)  # 直接添加字符串\n",
    "        else:\n",
    "            print(f\"Unexpected segment format: {segment}\")  # 记录其他格式\n",
    "    return words\n",
    "\n",
    "\n",
    "# 计算评价指标\n",
    "def calculate_metrics(gold_segments, predicted_segments):\n",
    "    gold_set = set(gold_segments)\n",
    "    predicted_set = set(predicted_segments)\n",
    "\n",
    "    TP = len(gold_set.intersection(predicted_set))  # True Positives\n",
    "    FP = len(predicted_set - gold_set)  # False Positives\n",
    "    FN = len(gold_set - predicted_set)  # False Negatives\n",
    "\n",
    "    precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "    recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "    return precision, recall, f1\n",
    "\n",
    "\n",
    "# 主函数\n",
    "def main(unsegmented_file, segmented_file):\n",
    "    unsegmented_text = read_file(unsegmented_file)\n",
    "    gold_text = read_file(segmented_file)\n",
    "\n",
    "    predicted_segments = segment_text(unsegmented_text)\n",
    "    gold_segments = gold_text.split()  # 假设已分词的文件以空格分隔\n",
    "\n",
    "    precision, recall, f1 = calculate_metrics(gold_segments, predicted_segments)\n",
    "\n",
    "    print(f'Precision: {precision:.4f}')\n",
    "    print(f'Recall: {recall:.4f}')\n",
    "    print(f'F1 Score: {f1:.4f}')\n",
    "\n",
    "\n",
    "# 示例调用\n",
    "# 请将以下路径替换为你的文件路径\n",
    "unsegmented_file = '../data/separate/msr_test.utf8'\n",
    "segmented_file = '../data/separate/msr_test_gold.utf8'\n",
    "\n",
    "# 运行主函数\n",
    "main(unsegmented_file, segmented_file)\n",
    "\n",
    "# 关闭分词器\n",
    "pynlpir.close()"
   ],
   "id": "1b1399f242132758",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## LTP",
   "id": "247a3c80309d4fd8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from ltp import LTP\n",
    "\n",
    "# 初始化 LTP\n",
    "ltp = LTP(\"LTP/small\")  # 加载 Small 模型\n",
    "\n",
    "\n",
    "# 读取文件内容\n",
    "def read_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        return f.read().strip()\n",
    "\n",
    "\n",
    "# 分词函数\n",
    "def segment_text(text):\n",
    "    output = ltp.pipeline([text], tasks=[\"cws\"])  # 分词，返回结果\n",
    "    return output.cws[0]  # 返回第一个元素，即分词结果\n",
    "\n",
    "\n",
    "# 计算评价指标\n",
    "def calculate_metrics(gold_segments, predicted_segments):\n",
    "    gold_set = set(gold_segments)\n",
    "    predicted_set = set(predicted_segments)\n",
    "\n",
    "    TP = len(gold_set.intersection(predicted_set))  # True Positives\n",
    "    FP = len(predicted_set - gold_set)  # False Positives\n",
    "    FN = len(gold_set - predicted_set)  # False Negatives\n",
    "\n",
    "    # print(TP, FP, FN)\n",
    "\n",
    "    precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "    recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "    return precision, recall, f1\n",
    "\n",
    "\n",
    "# 主函数\n",
    "def main(unsegmented_file, segmented_file):\n",
    "    unsegmented_text = read_file(unsegmented_file)\n",
    "    gold_text = read_file(segmented_file)\n",
    "\n",
    "    predicted_segments = segment_text(unsegmented_text)\n",
    "    gold_segments = gold_text.split()  # 假设已分词的文件以空格分隔\n",
    "\n",
    "    precision, recall, f1 = calculate_metrics(gold_segments, predicted_segments)\n",
    "\n",
    "    print(f'Precision: {precision:.4f}')\n",
    "    print(f'Recall: {recall:.4f}')\n",
    "    print(f'F1 Score: {f1:.4f}')\n",
    "\n",
    "\n",
    "# 示例调用\n",
    "# 请将以下路径替换为你的文件路径\n",
    "unsegmented_file = '../data/separate/msr_test.utf8'\n",
    "segmented_file = '../data/separate/msr_test_gold.utf8'\n",
    "\n",
    "# 运行主函数\n",
    "main(unsegmented_file, segmented_file)\n",
    "\n",
    "# 关闭 LTP\n",
    "# ltp.release()  # 释放模型资源\n"
   ],
   "id": "d8f2682e6db17f73",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## SpaCy",
   "id": "3eda877385cdb733"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import spacy\n",
    "\n",
    "# 加载中文模型\n",
    "nlp = spacy.load(\"zh_core_web_sm\")\n",
    "\n",
    "\n",
    "# 读取文件内容\n",
    "def read_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        return f.read().strip()\n",
    "\n",
    "\n",
    "# 分词函数\n",
    "def segment_text(text):\n",
    "    doc = nlp(text)  # 使用 SpaCy 处理文本\n",
    "    return [token.text for token in doc]  # 提取分词结果\n",
    "\n",
    "\n",
    "# 计算评价指标\n",
    "def calculate_metrics(gold_segments, predicted_segments):\n",
    "    gold_set = set(gold_segments)\n",
    "    predicted_set = set(predicted_segments)\n",
    "\n",
    "    TP = len(gold_set.intersection(predicted_set))  # True Positives\n",
    "    FP = len(predicted_set - gold_set)  # False Positives\n",
    "    FN = len(gold_set - predicted_set)  # False Negatives\n",
    "\n",
    "    precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "    recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "    return precision, recall, f1\n",
    "\n",
    "\n",
    "# 主函数\n",
    "def main(unsegmented_file, segmented_file):\n",
    "    unsegmented_text = read_file(unsegmented_file)\n",
    "    gold_text = read_file(segmented_file)\n",
    "\n",
    "    predicted_segments = segment_text(unsegmented_text)\n",
    "    gold_segments = gold_text.split()  # 假设已分词的文件以空格分隔\n",
    "\n",
    "    precision, recall, f1 = calculate_metrics(gold_segments, predicted_segments)\n",
    "\n",
    "    print(f'Precision: {precision:.4f}')\n",
    "    print(f'Recall: {recall:.4f}')\n",
    "    print(f'F1 Score: {f1:.4f}')\n",
    "\n",
    "\n",
    "# 示例调用\n",
    "# 请将以下路径替换为你的文件路径\n",
    "unsegmented_file = '../data/separate/msr_test.utf8'\n",
    "segmented_file = '../data/separate/msr_test_gold.utf8'\n",
    "\n",
    "# 运行主函数\n",
    "main(unsegmented_file, segmented_file)\n"
   ],
   "id": "8e54b9fc0d544e2b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 合并",
   "id": "b930ace853222e6f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pynlpir\n",
    "from ltp import LTP\n",
    "# from pyhanlp import *\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# 定义分词工具类\n",
    "class Tokenizer:\n",
    "    def segment(self, text):\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class PyNLPIRTokenizer(Tokenizer):\n",
    "    def __init__(self):\n",
    "        pynlpir.open()\n",
    "\n",
    "    def segment(self, text):\n",
    "        segments = pynlpir.segment(text, pos_tagging=False)\n",
    "        words = [segment[0] if isinstance(segment, tuple) else segment for segment in segments]\n",
    "        return words\n",
    "\n",
    "    def close(self):\n",
    "        pynlpir.close()\n",
    "\n",
    "\n",
    "class LTPTokenizer(Tokenizer):\n",
    "    def __init__(self):\n",
    "        self.ltp = LTP(\"LTP/small\")\n",
    "\n",
    "    def segment(self, text):\n",
    "        output = self.ltp.pipeline([text], tasks=[\"cws\"])\n",
    "        return output.cws[0]\n",
    "\n",
    "    def close(self):\n",
    "        self.ltp.release()\n",
    "\n",
    "\n",
    "class SpaCyTokenizer(Tokenizer):\n",
    "    def __init__(self):\n",
    "        self.nlp = spacy.load(\"zh_core_web_sm\")\n",
    "\n",
    "    def segment(self, text):\n",
    "        doc = self.nlp(text)\n",
    "        return [token.text for token in doc]\n",
    "\n",
    "\n",
    "# 计算评价指标\n",
    "def calculate_metrics(gold_segments, predicted_segments):\n",
    "    gold_set = set(gold_segments)\n",
    "    predicted_set = set(predicted_segments)\n",
    "\n",
    "    TP = len(gold_set.intersection(predicted_set))  # True Positives\n",
    "    FP = len(predicted_set - gold_set)  # False Positives\n",
    "    FN = len(gold_set - predicted_set)  # False Negatives\n",
    "\n",
    "    precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "    recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "    return precision, recall, f1\n",
    "\n",
    "\n",
    "# 主函数\n",
    "def main(unsegmented_file, segmented_file):\n",
    "    unsegmented_text = read_file(unsegmented_file)\n",
    "    gold_text = read_file(segmented_file)\n",
    "    gold_segments = gold_text.split()  # 假设已分词的文件以空格分隔\n",
    "\n",
    "    tokenizers = {\n",
    "        \"PyNLPIR\": PyNLPIRTokenizer(),\n",
    "        \"LTP\": LTPTokenizer(),\n",
    "        \"SpaCy\": SpaCyTokenizer()\n",
    "    }\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for name, tokenizer in tokenizers.items():\n",
    "        predicted_segments = tokenizer.segment(unsegmented_text)\n",
    "        precision, recall, f1 = calculate_metrics(gold_segments, predicted_segments)\n",
    "        results.append({\n",
    "            \"Tokenizer\": name,\n",
    "            \"Precision\": precision,\n",
    "            \"Recall\": recall,\n",
    "            \"F1 Score\": f1\n",
    "        })\n",
    "\n",
    "    # # 关闭所有分词器\n",
    "    # for tokenizer in tokenizers.values():\n",
    "    #     tokenizer.close()\n",
    "\n",
    "    # 创建 DataFrame\n",
    "    df = pd.DataFrame(results)\n",
    "    print(df)\n",
    "\n",
    "    # 设置索引为 Tokenizer\n",
    "    df.set_index('Tokenizer', inplace=True)\n",
    "\n",
    "    # 转置 DataFrame 以便于绘图\n",
    "    df_transposed = df.T\n",
    "\n",
    "    # 可视化\n",
    "    df_transposed.plot(kind='bar', figsize=(12, 6))\n",
    "\n",
    "    # 添加标题和标签\n",
    "    plt.title('Tokenization Performance Comparison')\n",
    "    plt.ylabel('Score')\n",
    "    plt.xticks(rotation=0)\n",
    "    plt.legend(title='Tokenizer', loc='upper left')\n",
    "    plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# 读取文件内容\n",
    "def read_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        return f.read().strip()\n",
    "\n",
    "\n",
    "# 示例调用\n",
    "unsegmented_file = '../data/separate/msr_test.utf8'\n",
    "segmented_file = '../data/separate/msr_test_gold.utf8'\n",
    "\n",
    "# 运行主函数\n",
    "main(unsegmented_file, segmented_file)\n"
   ],
   "id": "62b3cdf06d364199",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "61d36c15dccef6b5",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
